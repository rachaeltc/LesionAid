{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co4s-SH_8kzV",
    "outputId": "ccf4967c-e7a7-4b09-bacd-5bf8c38bb984"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y47tyenBKLCT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip \"/Users/sarahcasale/Downloads/LesionAid-main/data/aug_balanced_imgs.zip\"\n",
    "#!unzip \"/content/drive/MyDrive/DS4440_Project/preprocessed_images.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "USpUJJPvMwiF"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch import device\n",
    "from torch.cuda import is_available, device_count\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "l6XD1SXBaqP1",
    "outputId": "71a6d801-f9c2-41ef-8e21-051d6a576384"
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "DEVICE = device(\"cuda\" if is_available() else \"cpu\")  # (GPU if available)\n",
    "\n",
    "nz: int = 128  # length of latent vector\n",
    "ngf: int = 64  # depth of feature maps carried through the generator.\n",
    "ndf: int = 32  # depth of feature maps propagated through the discriminator\n",
    "nc: int = 3  # number of color channels (for color images = 3)\n",
    "# niter: int = NUM_EPOCHS BATCH_SIZE  # 300\n",
    "n_dnn: int = 32  # number of output features of the label's linear\n",
    "\n",
    "# Image and Label Constants\n",
    "IMAGE_SIZE: int = 64  # 128\n",
    "LABEL_TO_CLASS: dict = {\n",
    "    'N': 0,\n",
    "    'D': 1,\n",
    "    'G': 2,\n",
    "    'C': 3,\n",
    "    'A': 4,\n",
    "    'H': 5,\n",
    "    'M': 6,\n",
    "    'O': 7\n",
    "}\n",
    "LABEL_TO_TITLE: dict = {\n",
    "    'N': \"Normal\",\n",
    "    'D': \"Diabetes\",\n",
    "    'G': \"Galucoma\",\n",
    "    'C': \"Cataract\",\n",
    "    'A': \"Age related Macular Degeneration\",\n",
    "    'H': \"Hypertension\",\n",
    "    'M': \"Pathological Myopia\",\n",
    "    'O': \"Other diseases/abnormalities\"\n",
    "}\n",
    "CLASS_TO_LABEL: dict = {_v: _k for _k, _v in LABEL_TO_CLASS.items()}\n",
    "NUM_CLASSES: int = len(LABEL_TO_CLASS)\n",
    "\n",
    "# ---------------------\n",
    "# Training parameters\n",
    "# ---------------------\n",
    "BATCH_SIZE: int = 64 if IMAGE_SIZE == 64 else 32\n",
    "NUM_EPOCHS: int = 200\n",
    "\n",
    "# ---------------------\n",
    "# Hyperparameters\n",
    "# ---------------------\n",
    "# kept same hyperparameters as https://arxiv.org/pdf/1511.06434.pdf\n",
    "LEARNING_RATE: float = 0.0002\n",
    "BETA_1: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CustomDataset Class\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, root_dir, label_file, transform=None, label=\"ALL\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        if self.label == \"ALL\":\n",
    "          self.labels_df = pd.read_csv(label_file)\n",
    "        else: # only load images of the given label\n",
    "          label_df = pd.read_csv(label_file)\n",
    "          label_df = label_df[label_df['labels']==self.label].reset_index(drop=True)\n",
    "          self.labels_df = label_df\n",
    "        self.image_files = os.listdir(root_dir)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label from the label file\n",
    "        filename = self.image_files[idx]\n",
    "        if self.label=='ALL':\n",
    "          label = self.labels_df.loc[self.labels_df['filename'] == filename]['labels'].item()\n",
    "        else:\n",
    "          label = self.label\n",
    "        # Convert label to label class, to get index for encoding\n",
    "        label_ind = LABEL_TO_CLASS[label]\n",
    "        # one hot encoding for labels:\n",
    "        label_encoded = torch.zeros(NUM_CLASSES)\n",
    "        label_encoded[label_ind] = 1\n",
    "\n",
    "        return image, label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XJkUSrBTgFw"
   },
   "source": [
    "## GAN\n",
    "\n",
    "Adapted Code from [gcastro-98/synthetic-medical-images](https://github.com/gcastro-98/synthetic-medical-images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcrnrnJgjZgH"
   },
   "source": [
    "### `synthetic-medical-images` GAN\n",
    "Structure and Code from [gcastro-98/synthetic-medical-images](https://github.com/gcastro-98/synthetic-medical-images):\n",
    "- `Generator64` Class\n",
    "- `Discriminator64` Class\n",
    "- `train_gan` method\n",
    "- `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "dtrPWw45jYNq"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        assert IMAGE_SIZE == 64, \\\n",
    "            f\"This architecture is not suitable for IMAGE_SIZE = {IMAGE_SIZE}\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.y_label = nn.Sequential(\n",
    "            nn.Linear(NUM_CLASSES, n_dnn),  # 120, 1000\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.yz = nn.Sequential(\n",
    "            nn.Linear(nz, 2 * nz),  # 100, 200\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n_dnn + 2 * nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # mapping noise and label\n",
    "        z = self.yz(z)\n",
    "        y = self.y_label(y)\n",
    "\n",
    "        # mapping concatenated input to the main generator network\n",
    "        inp = torch.cat([z, y], 1)\n",
    "        inp = inp.view(-1, n_dnn + 2 * nz, 1, 1)  # 1000 + 200\n",
    "        output = self.main(inp)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        assert IMAGE_SIZE == 64, \\\n",
    "            f\"This architecture is not suitable for IMAGE_SIZE = {IMAGE_SIZE}\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        # self.ngpu = _ngpu\n",
    "        self.y_label = nn.Sequential(\n",
    "            nn.Linear(NUM_CLASSES, 64 * 64 * 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = self.y_label(y)\n",
    "        y = y.view(-1, 1, 64, 64)\n",
    "        inp = torch.cat([x, y], 1)\n",
    "        output = self.main(inp)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Z3Jhn1FzYF-H"
   },
   "outputs": [],
   "source": [
    "def __generate_random_noise():\n",
    "    return torch.randn(BATCH_SIZE, nz, device=DEVICE)\n",
    "\n",
    "\n",
    "def __generate_random_labels():\n",
    "    label = torch.zeros(BATCH_SIZE, NUM_CLASSES, device=DEVICE)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        x = np.random.randint(0, NUM_CLASSES)\n",
    "        label[i][x] = 1\n",
    "    return label\n",
    "\n",
    "_checkpoint_noise = __generate_random_noise()\n",
    "_checkpoint_labels = __generate_random_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "rmrdWTgVYYvA"
   },
   "outputs": [],
   "source": [
    "def _plot_losses(g_losses, d_losses,\n",
    "                 _show: bool = False) -> None:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(g_losses, label=\"Generator\")\n",
    "    plt.plot(d_losses, label=\"Discriminator\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join('.img', 'losses.png'), dpi=200)\n",
    "    if _show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "fL_v7cGGUul3"
   },
   "outputs": [],
   "source": [
    "def train_gan(\n",
    "        data_loader, use_cpu: bool = False,\n",
    "        save_best_model: bool = True, save_generated_images: bool = True,\n",
    "        verbose: bool = False, _freq: int = 5):\n",
    "    #DEVICE = device('cpu') if use_cpu else _DEVICE\n",
    "    # initialize (with weights) generator and discriminator\n",
    "    net_g = Generator().to(DEVICE)\n",
    "    #net_g.apply(weights_init)\n",
    "    net_d = Discriminator().to(DEVICE)\n",
    "    #net_d.apply(weights_init)\n",
    "\n",
    "    # loss function and optimizers\n",
    "    criterion = nn.BCELoss()  # we are simply detecting whether it's real/fake\n",
    "\n",
    "    real_label = float(1)\n",
    "    fake_label = float(0)\n",
    "\n",
    "    # setup optimizer\n",
    "    optimizer_d = optim.Adam(\n",
    "        net_d.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
    "    optimizer_g = optim.Adam(\n",
    "        net_g.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
    "    d_error_epoch = []\n",
    "    g_error_epoch = []\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        # we will start iterating each batch element\n",
    "        d_error_iter = 0\n",
    "        g_error_iter = 0\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            # DISCRIMINATOR\n",
    "            # train with real\n",
    "            net_d.zero_grad()\n",
    "            real_cpu = data[0].to(DEVICE)\n",
    "            batch_size = real_cpu.size(0)\n",
    "            pathology_one_hot = data[1].to(DEVICE)\n",
    "            label = torch.full((batch_size, ), real_label, device=DEVICE)\n",
    "\n",
    "            output = net_d(real_cpu, pathology_one_hot)\n",
    "            err_d_real = criterion(output, label)\n",
    "            err_d_real.backward()\n",
    "            # D_x = output.mean().item()\n",
    "\n",
    "            # train with fake\n",
    "            noise = torch.randn(batch_size, nz, device=DEVICE)\n",
    "            fake = net_g(noise, pathology_one_hot)\n",
    "            label.fill_(fake_label)\n",
    "            output = net_d(fake.detach(), pathology_one_hot)\n",
    "            err_d_fake = criterion(output, label)\n",
    "            err_d_fake.backward()\n",
    "            # D_G_z1 = output.mean().item()\n",
    "            err_d = err_d_real + err_d_fake\n",
    "            d_error_iter += err_d.item()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # GENERATOR\n",
    "            net_g.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            output = net_d(fake, pathology_one_hot)\n",
    "            err_g = criterion(output, label)\n",
    "            g_error_iter += err_g.item()\n",
    "            err_g.backward()\n",
    "            # D_G_z2 = output.mean().item()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            if (i + 1) % (BATCH_SIZE // 4) == 0 and verbose:\n",
    "                # we print the losses\n",
    "                _counter = f\"Epoch [{epoch}/{NUM_EPOCHS}]\" \\\n",
    "                           f\"[{i}/{len(data_loader)}]\"\n",
    "                print(f\"{_counter} --- Loss G: {err_g.item()}\")\n",
    "                print(f\"{_counter} --- Loss D: {err_d.item()}\")\n",
    "\n",
    "        if (epoch + 1) % _freq == 0:\n",
    "            # we save generated images\n",
    "            with torch.no_grad():\n",
    "                if save_generated_images:\n",
    "                    print(\n",
    "                        f\"CHECKPOINT {epoch + 1}: saving some generated images at 'output/' directory\")\n",
    "                    checkpoint_images = net_g(\n",
    "                        _checkpoint_noise, _checkpoint_labels)\n",
    "                    # we re-scale generated images to [0, 1] and save them\n",
    "                    save_image((checkpoint_images + 1) / 2,\n",
    "                               f\"output/epoch_{epoch + 1}.png\", nrow=8, normalize=True)\n",
    "\n",
    "            # save models as checkpoint\n",
    "            if save_best_model:\n",
    "                print(f\"CHECKPOINT {epoch + 1}: saving the trained\"\n",
    "                             \" models at 'models/' directory\")\n",
    "                torch.save(net_g.state_dict(),\n",
    "                           \"models/generator.pth\")\n",
    "                torch.save(net_d.state_dict(),\n",
    "                           \"models/discriminator.pth\")\n",
    "\n",
    "        # accumulate error for each epoch\n",
    "        d_error_epoch.append(d_error_iter)\n",
    "        g_error_epoch.append(g_error_iter)\n",
    "\n",
    "    _plot_losses(g_error_epoch, d_error_epoch)\n",
    "\n",
    "    # save the trained generator\n",
    "    torch.save(net_g.state_dict(),\n",
    "               \"models/generator.pth\")\n",
    "    # as well as the trained discriminator\n",
    "    torch.save(net_d.state_dict(),\n",
    "               \"models/discriminator.pth\")\n",
    "\n",
    "    return net_g, net_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "qSd-gUGYZCWo"
   },
   "outputs": [],
   "source": [
    "def plot_fake_images(\n",
    "        generator, n_images: int = 9, _show: bool = False) -> None:\n",
    "    cols, rows = 3, 3\n",
    "    fig, axs = plt.subplots(rows, cols, sharex='all')\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    gen_z, label, _label_names = __generate_random_inputs(n_images)\n",
    "    gen_images = generator(gen_z, label)\n",
    "    images = gen_images.to(\"cpu\").clone().detach()\n",
    "    images = images.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    for i in range(9):\n",
    "        axs[i].set_title(_label_names[i])\n",
    "        axs[i].set_axis_off()\n",
    "        axs[i].imshow(images[i])\n",
    "    plt.tight_layout(pad=1.04)\n",
    "    plt.savefig(os.path.join('.img', 'fake_samples.png'), dpi=200)\n",
    "    if _show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def __generate_random_inputs(n_images: int):\n",
    "    gen_z = torch.randn(n_images, nz, device=DEVICE)\n",
    "    label = torch.zeros(n_images, NUM_CLASSES, device=DEVICE)\n",
    "    _label_names = []\n",
    "    for i in range(n_images):\n",
    "        x = np.random.randint(0, NUM_CLASSES)\n",
    "        label[i][x] = 1\n",
    "        _label_names.append(LABEL_TO_TITLE[CLASS_TO_LABEL[x]])\n",
    "    return gen_z, label, _label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    print(\"Loading already trained and serialized generator model\")\n",
    "    generator = Generator().to(device(DEVICE))\n",
    "    generator.load_state_dict(torch.load(\n",
    "        os.path.join(\"models/generator.pth\")))\n",
    "    plot_fake_images(generator, _show=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ryKRfqa8a8Wf"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "#dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
    "dataset = CustomDataset(root_dir='../data/original/preprocessed_images', label_file='../data/original/odir_labels.csv', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRiIj6xnZKBU",
    "outputId": "e6642fd2-bae6-47e8-f76a-82db83df6496"
   },
   "outputs": [],
   "source": [
    "generator, discriminator = train_gan(dataloader)\n",
    "plot_fake_images(generator, _show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203c14a492a54bcea5e8d44c52ae481a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 64 # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"occularaid-M\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"cheungra/occularaid-M\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "#dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
    "dataset = CustomDataset(root_dir='../data/preprocessed_images', label_file='../data/odir_labels.csv', transform=transform, label='M')\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "timesteps = torch.LongTensor([50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[0]\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ed18a51ecb4ff599e0a6a66d16b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/cheungra/ocularaid-diffusion/commit/99e708d0536305f11d6d941e00ae14a066f2e036', commit_message='Upload model', commit_description='', oid='99e708d0536305f11d6d941e00ae14a066f2e036', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to huggingface hub\n",
    "model.push_to_hub(\"ocularaid-diffusion\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_config('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cheungra/ocularaid-diffusion does not appear to have a file named config.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/cheungra/ocularaid-diffusion/resolve/main/unet/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\diffusers\\configuration_utils.py:380\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[1;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    381\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    382\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name,\n\u001b[0;32m    383\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    384\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    385\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    386\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    387\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    388\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    389\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    390\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    391\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1262\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1263\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1264\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1265\u001b[0m         timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1266\u001b[0m         library_name\u001b[38;5;241m=\u001b[39mlibrary_name,\n\u001b[0;32m   1267\u001b[0m         library_version\u001b[38;5;241m=\u001b[39mlibrary_version,\n\u001b[0;32m   1268\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1675\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1676\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1677\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1678\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1679\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1680\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1681\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1682\u001b[0m )\n\u001b[0;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    370\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    371\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    372\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 393\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-661e2f3e-15bf3b0c303278d47572b016;924797df-178d-41df-8db5-046ab80784b8)\n\nEntry Not Found for url: https://huggingface.co/cheungra/ocularaid-diffusion/resolve/main/unet/config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet2DConditionModel\n\u001b[0;32m      3\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheungra/ocularaid-diffusion\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m unet \u001b[38;5;241m=\u001b[39m UNet2DConditionModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(repo_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_embed_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojection\u001b[39m\u001b[38;5;124m\"\u001b[39m,projection_class_embeddings_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:567\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffusers\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# load config\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m config, unused_kwargs, commit_hash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_config(\n\u001b[0;32m    568\u001b[0m     config_path,\n\u001b[0;32m    569\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    570\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    571\u001b[0m     return_commit_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    573\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    574\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    575\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    576\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    577\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    578\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    579\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m    580\u001b[0m     max_memory\u001b[38;5;241m=\u001b[39mmax_memory,\n\u001b[0;32m    581\u001b[0m     offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m    582\u001b[0m     offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m    583\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    585\u001b[0m )\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[0;32m    588\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\diffusers\\configuration_utils.py:406\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[1;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this model name. Check the model page at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     )\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: cheungra/ocularaid-diffusion does not appear to have a file named config.json."
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "repo_id = \"cheungra/ocularaid-diffusion\"\n",
    "unet = UNet2DConditionModel.from_pretrained(repo_id, subfolder=\"unet\", class_embed_type = \"projection\",projection_class_embeddings_input_dim=16,revision=None, low_cpu_mem_usage=False, device_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
