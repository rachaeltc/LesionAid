{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co4s-SH_8kzV",
        "outputId": "ccf4967c-e7a7-4b09-bacd-5bf8c38bb984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y47tyenBKLCT"
      },
      "outputs": [],
      "source": [
        "#!unzip \"/content/drive/My Drive/Colab Notebooks/ds4440_data.zip\"\n",
        "!unzip \"/content/drive/MyDrive/DS4440_Project/preprocessed_images.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "USpUJJPvMwiF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, ResNet50_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "l6XD1SXBaqP1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "71a6d801-f9c2-41ef-8e21-051d6a576384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n",
            "torch.Size([3, 28, 28])\n",
            "torch.Size([3, 28, 28])\n",
            "torch.Size([3, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: ('D', 'C', 'M', 'N')\n"
          ]
        }
      ],
      "source": [
        "# Tests that the Data is Properly Loaded by Printing\n",
        "def show_images(images):\n",
        "    fig, axs = plt.subplots(1, len(images), figsize=(12, 3))\n",
        "    for i, image in enumerate(images):\n",
        "        # Normalize image to [0, 1]\n",
        "        image = image - image.min()\n",
        "        image = image / image.max()\n",
        "        if image.shape[0] == 1:\n",
        "            axs[i].imshow(image.squeeze(), cmap='gray')\n",
        "        else:\n",
        "            axs[i].imshow(image.permute(1, 2, 0))\n",
        "        axs[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "images, labels = next(iter(dataloader))\n",
        "show_images(images[:5])\n",
        "print(\"Labels:\", labels[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN\n",
        "\n",
        "Adapted Code from [gcastro-98/synthetic-medical-images](https://github.com/gcastro-98/synthetic-medical-images)\n",
        "\n"
      ],
      "metadata": {
        "id": "4XJkUSrBTgFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from torch import device\n",
        "from torch.cuda import is_available, device_count\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "inC4Zf7LjDo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONSTANTS\n",
        "\n",
        "DEVICE = device(\"cuda\" if is_available() else \"cpu\")  # (GPU if available)\n",
        "\n",
        "nz: int = 128  # length of latent vector\n",
        "ngf: int = 128  # depth of feature maps carried through the generator.\n",
        "ndf: int = 64  # depth of feature maps propagated through the discriminator\n",
        "nc: int = 3  # number of color channels (for color images = 3)\n",
        "# niter: int = NUM_EPOCHS BATCH_SIZE  # 300\n",
        "n_dnn: int = 64  # number of output features of the label's linear\n",
        "\n",
        "# Image and Label Constants\n",
        "IMAGE_SIZE: int = 64  # 128\n",
        "LABEL_TO_CLASS: dict = {\n",
        "    'N': 0,\n",
        "    'D': 1,\n",
        "    'G': 2,\n",
        "    'C': 3,\n",
        "    'A': 4,\n",
        "    'H': 5,\n",
        "    'M': 6,\n",
        "    'O': 7\n",
        "}\n",
        "LABEL_TO_TITLE: dict = {\n",
        "    'N': \"Normal\",\n",
        "    'D': \"Diabetes\",\n",
        "    'G': \"Galucoma\",\n",
        "    'C': \"Cataract\",\n",
        "    'A': \"Age related Macular Degeneration\",\n",
        "    'H': \"Hypertension\",\n",
        "    'M': \"Pathological Myopia\",\n",
        "    'O': \"Other diseases/abnormalities\"\n",
        "}\n",
        "CLASS_TO_LABEL: dict = {_v: _k for _k, _v in LABEL_TO_CLASS.items()}\n",
        "NUM_CLASSES: int = len(LABEL_TO_CLASS)\n",
        "\n",
        "# ---------------------\n",
        "# Training parameters\n",
        "# ---------------------\n",
        "BATCH_SIZE: int = 64 if IMAGE_SIZE == 64 else 32\n",
        "NUM_EPOCHS: int = 200\n",
        "\n",
        "# ---------------------\n",
        "# Hyperparameters\n",
        "# ---------------------\n",
        "# kept same hyperparameters as https://arxiv.org/pdf/1511.06434.pdf\n",
        "LEARNING_RATE: float = 0.0002\n",
        "BETA_1: float = 0.5"
      ],
      "metadata": {
        "id": "S0DMC-adcOVx"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CustomDataset Class\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, root_dir, label_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "        self.labels_df = pd.read_csv(label_file)\n",
        "        self.image_files = os.listdir(root_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Extract label from the label file\n",
        "        filename = self.image_files[idx]\n",
        "        label = self.labels_df.loc[self.labels_df['filename'] == filename]['labels'].item()\n",
        "        # Convert label to label class, to get index for encoding\n",
        "        label_ind = LABEL_TO_CLASS[label]\n",
        "        # one hot encoding for labels:\n",
        "        label_encoded = torch.zeros(NUM_CLASSES)\n",
        "        label_encoded[label_ind] = 1\n",
        "\n",
        "        return image, label_encoded"
      ],
      "metadata": {
        "id": "OzG9OMG3Tecy"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `synthetic-medical-images` GAN\n",
        "Structure and Code from [gcastro-98/synthetic-medical-images](https://github.com/gcastro-98/synthetic-medical-images):\n",
        "- `Generator64` Class\n",
        "- `Discriminator64` Class\n",
        "- `train_gan` method\n",
        "- `"
      ],
      "metadata": {
        "id": "FcrnrnJgjZgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        assert IMAGE_SIZE == 64, \\\n",
        "            f\"This architecture is not suitable for IMAGE_SIZE = {IMAGE_SIZE}\"\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.y_label = nn.Sequential(\n",
        "            nn.Linear(NUM_CLASSES, n_dnn),  # 120, 1000\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.yz = nn.Sequential(\n",
        "            nn.Linear(nz, 2 * nz),  # 100, 200\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_dnn + 2 * nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        # mapping noise and label\n",
        "        z = self.yz(z)\n",
        "        y = self.y_label(y)\n",
        "\n",
        "        # mapping concatenated input to the main generator network\n",
        "        inp = torch.cat([z, y], 1)\n",
        "        inp = inp.view(-1, n_dnn + 2 * nz, 1, 1)  # 1000 + 200\n",
        "        output = self.main(inp)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        assert IMAGE_SIZE == 64, \\\n",
        "            f\"This architecture is not suitable for IMAGE_SIZE = {IMAGE_SIZE}\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        # self.ngpu = _ngpu\n",
        "        self.y_label = nn.Sequential(\n",
        "            nn.Linear(NUM_CLASSES, 64 * 64 * 1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        y = self.y_label(y)\n",
        "        y = y.view(-1, 1, 64, 64)\n",
        "        inp = torch.cat([x, y], 1)\n",
        "        output = self.main(inp)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)"
      ],
      "metadata": {
        "id": "dtrPWw45jYNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __generate_random_noise():\n",
        "    return torch.randn(BATCH_SIZE, nz, device=DEVICE)\n",
        "\n",
        "\n",
        "def __generate_random_labels():\n",
        "    label = torch.zeros(BATCH_SIZE, NUM_CLASSES, device=DEVICE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        x = np.random.randint(0, NUM_CLASSES)\n",
        "        label[i][x] = 1\n",
        "    return label\n",
        "\n",
        "_checkpoint_noise = __generate_random_noise()\n",
        "_checkpoint_labels = __generate_random_labels()"
      ],
      "metadata": {
        "id": "Z3Jhn1FzYF-H"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _plot_losses(g_losses, d_losses,\n",
        "                 _show: bool = False) -> None:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(g_losses, label=\"Generator\")\n",
        "    plt.plot(d_losses, label=\"Discriminator\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('.img', 'losses.png'), dpi=200)\n",
        "    if _show:\n",
        "        plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "rmrdWTgVYYvA"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(\n",
        "        data_loader, use_cpu: bool = False,\n",
        "        save_best_model: bool = True, save_generated_images: bool = True,\n",
        "        verbose: bool = False, _freq: int = 5):\n",
        "    #DEVICE = device('cpu') if use_cpu else _DEVICE\n",
        "    # initialize (with weights) generator and discriminator\n",
        "    net_g = Generator().to(DEVICE)\n",
        "    #net_g.apply(weights_init)\n",
        "    net_d = Discriminator().to(DEVICE)\n",
        "    #net_d.apply(weights_init)\n",
        "\n",
        "    # loss function and optimizers\n",
        "    criterion = nn.BCELoss()  # we are simply detecting whether it's real/fake\n",
        "\n",
        "    real_label = float(1)\n",
        "    fake_label = float(0)\n",
        "\n",
        "    # setup optimizer\n",
        "    optimizer_d = optim.Adam(\n",
        "        net_d.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
        "    optimizer_g = optim.Adam(\n",
        "        net_g.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
        "    d_error_epoch = []\n",
        "    g_error_epoch = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # we will start iterating each batch element\n",
        "        d_error_iter = 0\n",
        "        g_error_iter = 0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            # DISCRIMINATOR\n",
        "            # train with real\n",
        "            net_d.zero_grad()\n",
        "            real_cpu = data[0].to(DEVICE)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            pathology_one_hot = data[1].to(DEVICE)\n",
        "            label = torch.full((batch_size, ), real_label, device=DEVICE)\n",
        "\n",
        "            output = net_d(real_cpu, pathology_one_hot)\n",
        "            err_d_real = criterion(output, label)\n",
        "            err_d_real.backward()\n",
        "            # D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, device=DEVICE)\n",
        "            fake = net_g(noise, pathology_one_hot)\n",
        "            label.fill_(fake_label)\n",
        "            output = net_d(fake.detach(), pathology_one_hot)\n",
        "            err_d_fake = criterion(output, label)\n",
        "            err_d_fake.backward()\n",
        "            # D_G_z1 = output.mean().item()\n",
        "            err_d = err_d_real + err_d_fake\n",
        "            d_error_iter += err_d.item()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # GENERATOR\n",
        "            net_g.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = net_d(fake, pathology_one_hot)\n",
        "            err_g = criterion(output, label)\n",
        "            g_error_iter += err_g.item()\n",
        "            err_g.backward()\n",
        "            # D_G_z2 = output.mean().item()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            if (i + 1) % (BATCH_SIZE // 4) == 0 and verbose:\n",
        "                # we print the losses\n",
        "                _counter = f\"Epoch [{epoch}/{NUM_EPOCHS}]\" \\\n",
        "                           f\"[{i}/{len(data_loader)}]\"\n",
        "                print(f\"{_counter} --- Loss G: {err_g.item()}\")\n",
        "                print(f\"{_counter} --- Loss D: {err_d.item()}\")\n",
        "\n",
        "        if (epoch + 1) % _freq == 0:\n",
        "            # we save generated images\n",
        "            with torch.no_grad():\n",
        "                if save_generated_images:\n",
        "                    print(\n",
        "                        \"CHECKPOINT: saving some generated images at 'output/' directory\")\n",
        "                    checkpoint_images = net_g(\n",
        "                        _checkpoint_noise, _checkpoint_labels)\n",
        "                    # we re-scale generated images to [0, 1] and save them\n",
        "                    save_image((checkpoint_images + 1) / 2,\n",
        "                               f\"output/epoch_{epoch + 1}.png\", nrow=8, normalize=True)\n",
        "\n",
        "            # save models as checkpoint\n",
        "            if save_best_model:\n",
        "                print(\"CHECKPOINT: saving the trained\"\n",
        "                             f\" models at 'models/' directory\")\n",
        "                torch.save(net_g.state_dict(),\n",
        "                           \"models/generator.pth\")\n",
        "                torch.save(net_d.state_dict(),\n",
        "                           \"models/discriminator.pth\")\n",
        "\n",
        "        # accumulate error for each epoch\n",
        "        d_error_epoch.append(d_error_iter)\n",
        "        g_error_epoch.append(g_error_iter)\n",
        "\n",
        "    _plot_losses(g_error_epoch, d_error_epoch)\n",
        "\n",
        "    # save the trained generator\n",
        "    torch.save(net_g.state_dict(),\n",
        "               \"models/generator.pth\")\n",
        "    # as well as the trained discriminator\n",
        "    torch.save(net_d.state_dict(),\n",
        "               \"models/discriminator.pth\")\n",
        "\n",
        "    return net_g, net_d"
      ],
      "metadata": {
        "id": "fL_v7cGGUul3"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fake_images(\n",
        "        generator, n_images: int = 9, _show: bool = False) -> None:\n",
        "    cols, rows = 3, 3\n",
        "    fig, axs = plt.subplots(rows, cols, sharex='all')\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    gen_z, label, _label_names = __generate_random_inputs(n_images)\n",
        "    gen_images = generator(gen_z, label)\n",
        "    images = gen_images.to(\"cpu\").clone().detach()\n",
        "    images = images.numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "    for i in range(9):\n",
        "        axs[i].set_title(_label_names[i])\n",
        "        axs[i].set_axis_off()\n",
        "        axs[i].imshow(images[i])\n",
        "    plt.tight_layout(pad=1.04)\n",
        "    plt.savefig(os.path.join('.img', 'fake_samples.png'), dpi=200)\n",
        "    if _show:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def __generate_random_inputs(n_images: int):\n",
        "    gen_z = torch.randn(n_images, nz, device=DEVICE)\n",
        "    label = torch.zeros(n_images, NUM_CLASSES, device=DEVICE)\n",
        "    _label_names = []\n",
        "    for i in range(n_images):\n",
        "        x = np.random.randint(0, NUM_CLASSES)\n",
        "        label[i][x] = 1\n",
        "        _label_names.append(LABEL_TO_TITLE[CLASS_TO_LABEL[x]])\n",
        "    return gen_z, label, _label_names"
      ],
      "metadata": {
        "id": "qSd-gUGYZCWo"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "ryKRfqa8a8Wf"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator, discriminator = train_gan(dataloader)\n",
        "plot_fake_images(generator, _show=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRiIj6xnZKBU",
        "outputId": "e6642fd2-bae6-47e8-f76a-82db83df6496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n",
            "CHECKPOINT: saving some generated images at 'output/' directory\n",
            "CHECKPOINT: saving the trained models at 'models/' directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kzWGcPP5dhDV"
      },
      "outputs": [],
      "source": [
        "# Pretrained ViT for Image Classification\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=weights)\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}