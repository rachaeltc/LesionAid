{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co4s-SH_8kzV",
        "outputId": "ccf4967c-e7a7-4b09-bacd-5bf8c38bb984"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "#from google.colab import drive\n",
        "\n",
        "#drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y47tyenBKLCT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "#!unzip \"/content/drive/My Drive/Colab Notebooks/ds4440_data.zip\"\n",
        "!unzip \"/content/drive/MyDrive/DS4440_Project/preprocessed_images.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "USpUJJPvMwiF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "l6XD1SXBaqP1",
        "outputId": "71a6d801-f9c2-41ef-8e21-051d6a576384"
      },
      "outputs": [],
      "source": [
        "# Tests that the Data is Properly Loaded by Printing\n",
        "def show_images(images):\n",
        "    fig, axs = plt.subplots(1, len(images), figsize=(12, 3))\n",
        "    for i, image in enumerate(images):\n",
        "        # Normalize image to [0, 1]\n",
        "        image = image - image.min()\n",
        "        image = image / image.max()\n",
        "        if image.shape[0] == 1:\n",
        "            axs[i].imshow(image.squeeze(), cmap='gray')\n",
        "        else:\n",
        "            axs[i].imshow(image.permute(1, 2, 0))\n",
        "        axs[i].axis('off')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XJkUSrBTgFw"
      },
      "source": [
        "## GAN\n",
        "\n",
        "Adapted Code from [gcastro-98/synthetic-medical-images](https://github.com/gcastro-98/synthetic-medical-images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "inC4Zf7LjDo7"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torch import device\n",
        "from torch.cuda import is_available, device_count\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S0DMC-adcOVx"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "\n",
        "DEVICE = device(\"cuda\" if is_available() else \"cpu\")  # (GPU if available)\n",
        "\n",
        "nz: int = 128  # length of latent vector\n",
        "ngf: int = 128  # depth of feature maps carried through the generator.\n",
        "ndf: int = 64  # depth of feature maps propagated through the discriminator\n",
        "nc: int = 3  # number of color channels (for color images = 3)\n",
        "# niter: int = NUM_EPOCHS BATCH_SIZE  # 300\n",
        "n_dnn: int = 64  # number of output features of the label's linear\n",
        "\n",
        "# Image and Label Constants\n",
        "IMAGE_SIZE: int = 64  # 128\n",
        "LABEL_TO_CLASS: dict = {\n",
        "    'N': 0,\n",
        "    'D': 1,\n",
        "    'G': 2,\n",
        "    'C': 3,\n",
        "    'A': 4,\n",
        "    'H': 5,\n",
        "    'M': 6,\n",
        "    'O': 7\n",
        "}\n",
        "LABEL_TO_TITLE: dict = {\n",
        "    'N': \"Normal\",\n",
        "    'D': \"Diabetes\",\n",
        "    'G': \"Galucoma\",\n",
        "    'C': \"Cataract\",\n",
        "    'A': \"Age related Macular Degeneration\",\n",
        "    'H': \"Hypertension\",\n",
        "    'M': \"Pathological Myopia\",\n",
        "    'O': \"Other diseases/abnormalities\"\n",
        "}\n",
        "CLASS_TO_LABEL: dict = {_v: _k for _k, _v in LABEL_TO_CLASS.items()}\n",
        "NUM_CLASSES: int = len(LABEL_TO_CLASS)\n",
        "\n",
        "# ---------------------\n",
        "# Training parameters\n",
        "# ---------------------\n",
        "BATCH_SIZE: int = 64 if IMAGE_SIZE == 64 else 32\n",
        "NUM_EPOCHS: int = 200\n",
        "\n",
        "# ---------------------\n",
        "# Hyperparameters\n",
        "# ---------------------\n",
        "# kept same hyperparameters as https://arxiv.org/pdf/1511.06434.pdf\n",
        "LEARNING_RATE: float = 0.0002\n",
        "BETA_1: float = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OzG9OMG3Tecy"
      },
      "outputs": [],
      "source": [
        "# Create CustomDataset Class\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, root_dir, label_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "        self.labels_df = pd.read_csv(label_file)\n",
        "        self.image_files = os.listdir(root_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Extract label from the label file\n",
        "        filename = self.image_files[idx]\n",
        "        label = self.labels_df.loc[self.labels_df['filename'] == filename]['labels'].item()\n",
        "        # Convert label to label class, to get index for encoding\n",
        "        label_ind = LABEL_TO_CLASS[label]\n",
        "        # one hot encoding for labels:\n",
        "        label_encoded = torch.zeros(NUM_CLASSES)\n",
        "        label_encoded[label_ind] = 1\n",
        "\n",
        "        return image, label_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcrnrnJgjZgH"
      },
      "source": [
        "### `synthetic-medical-images` GAN\n",
        "Structure and Code from [gcastro-98/synthetic-medical-images](https://github.com/gcastro-98/synthetic-medical-images):\n",
        "- `Generator64` Class\n",
        "- `Discriminator64` Class\n",
        "- `train_gan` method\n",
        "- `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dtrPWw45jYNq"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        assert IMAGE_SIZE == 64, \\\n",
        "            f\"This architecture is not suitable for IMAGE_SIZE = {IMAGE_SIZE}\"\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.y_label = nn.Sequential(\n",
        "            nn.Linear(NUM_CLASSES, n_dnn),  # 120, 1000\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.yz = nn.Sequential(\n",
        "            nn.Linear(nz, 2 * nz),  # 100, 200\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_dnn + 2 * nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        # mapping noise and label\n",
        "        z = self.yz(z)\n",
        "        y = self.y_label(y)\n",
        "\n",
        "        # mapping concatenated input to the main generator network\n",
        "        inp = torch.cat([z, y], 1)\n",
        "        inp = inp.view(-1, n_dnn + 2 * nz, 1, 1)  # 1000 + 200\n",
        "        output = self.main(inp)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        assert IMAGE_SIZE == 64, \\\n",
        "            f\"This architecture is not suitable for IMAGE_SIZE = {IMAGE_SIZE}\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        # self.ngpu = _ngpu\n",
        "        self.y_label = nn.Sequential(\n",
        "            nn.Linear(NUM_CLASSES, 64 * 64 * 1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        y = self.y_label(y)\n",
        "        y = y.view(-1, 1, 64, 64)\n",
        "        inp = torch.cat([x, y], 1)\n",
        "        output = self.main(inp)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z3Jhn1FzYF-H"
      },
      "outputs": [],
      "source": [
        "def __generate_random_noise():\n",
        "    return torch.randn(BATCH_SIZE, nz, device=DEVICE)\n",
        "\n",
        "\n",
        "def __generate_random_labels():\n",
        "    label = torch.zeros(BATCH_SIZE, NUM_CLASSES, device=DEVICE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        x = np.random.randint(0, NUM_CLASSES)\n",
        "        label[i][x] = 1\n",
        "    return label\n",
        "\n",
        "_checkpoint_noise = __generate_random_noise()\n",
        "_checkpoint_labels = __generate_random_labels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rmrdWTgVYYvA"
      },
      "outputs": [],
      "source": [
        "def _plot_losses(g_losses, d_losses,\n",
        "                 _show: bool = False) -> None:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(g_losses, label=\"Generator\")\n",
        "    plt.plot(d_losses, label=\"Discriminator\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('.img', 'losses.png'), dpi=200)\n",
        "    if _show:\n",
        "        plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fL_v7cGGUul3"
      },
      "outputs": [],
      "source": [
        "def train_gan(\n",
        "        data_loader, use_cpu: bool = False,\n",
        "        save_best_model: bool = True, save_generated_images: bool = True,\n",
        "        verbose: bool = False, _freq: int = 5):\n",
        "    #DEVICE = device('cpu') if use_cpu else _DEVICE\n",
        "    # initialize (with weights) generator and discriminator\n",
        "    net_g = Generator().to(DEVICE)\n",
        "    #net_g.apply(weights_init)\n",
        "    net_d = Discriminator().to(DEVICE)\n",
        "    #net_d.apply(weights_init)\n",
        "\n",
        "    # loss function and optimizers\n",
        "    criterion = nn.BCELoss()  # we are simply detecting whether it's real/fake\n",
        "\n",
        "    real_label = float(1)\n",
        "    fake_label = float(0)\n",
        "\n",
        "    # setup optimizer\n",
        "    optimizer_d = optim.Adam(\n",
        "        net_d.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
        "    optimizer_g = optim.Adam(\n",
        "        net_g.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
        "    d_error_epoch = []\n",
        "    g_error_epoch = []\n",
        "\n",
        "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "        # we will start iterating each batch element\n",
        "        d_error_iter = 0\n",
        "        g_error_iter = 0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            # DISCRIMINATOR\n",
        "            # train with real\n",
        "            net_d.zero_grad()\n",
        "            real_cpu = data[0].to(DEVICE)\n",
        "            batch_size = real_cpu.size(0)\n",
        "            pathology_one_hot = data[1].to(DEVICE)\n",
        "            label = torch.full((batch_size, ), real_label, device=DEVICE)\n",
        "\n",
        "            output = net_d(real_cpu, pathology_one_hot)\n",
        "            err_d_real = criterion(output, label)\n",
        "            err_d_real.backward()\n",
        "            # D_x = output.mean().item()\n",
        "\n",
        "            # train with fake\n",
        "            noise = torch.randn(batch_size, nz, device=DEVICE)\n",
        "            fake = net_g(noise, pathology_one_hot)\n",
        "            label.fill_(fake_label)\n",
        "            output = net_d(fake.detach(), pathology_one_hot)\n",
        "            err_d_fake = criterion(output, label)\n",
        "            err_d_fake.backward()\n",
        "            # D_G_z1 = output.mean().item()\n",
        "            err_d = err_d_real + err_d_fake\n",
        "            d_error_iter += err_d.item()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # GENERATOR\n",
        "            net_g.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            output = net_d(fake, pathology_one_hot)\n",
        "            err_g = criterion(output, label)\n",
        "            g_error_iter += err_g.item()\n",
        "            err_g.backward()\n",
        "            # D_G_z2 = output.mean().item()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            if (i + 1) % (BATCH_SIZE // 4) == 0 and verbose:\n",
        "                # we print the losses\n",
        "                _counter = f\"Epoch [{epoch}/{NUM_EPOCHS}]\" \\\n",
        "                           f\"[{i}/{len(data_loader)}]\"\n",
        "                print(f\"{_counter} --- Loss G: {err_g.item()}\")\n",
        "                print(f\"{_counter} --- Loss D: {err_d.item()}\")\n",
        "\n",
        "        if (epoch + 1) % _freq == 0:\n",
        "            # we save generated images\n",
        "            with torch.no_grad():\n",
        "                if save_generated_images:\n",
        "                    print(\n",
        "                        f\"CHECKPOINT {epoch + 1}: saving some generated images at 'output/' directory\")\n",
        "                    checkpoint_images = net_g(\n",
        "                        _checkpoint_noise, _checkpoint_labels)\n",
        "                    # we re-scale generated images to [0, 1] and save them\n",
        "                    save_image((checkpoint_images + 1) / 2,\n",
        "                               f\"output/epoch_{epoch + 1}.png\", nrow=8, normalize=True)\n",
        "\n",
        "            # save models as checkpoint\n",
        "            if save_best_model:\n",
        "                print(f\"CHECKPOINT {epoch + 1}: saving the trained\"\n",
        "                             \" models at 'models/' directory\")\n",
        "                torch.save(net_g.state_dict(),\n",
        "                           \"models/generator.pth\")\n",
        "                torch.save(net_d.state_dict(),\n",
        "                           \"models/discriminator.pth\")\n",
        "\n",
        "        # accumulate error for each epoch\n",
        "        d_error_epoch.append(d_error_iter)\n",
        "        g_error_epoch.append(g_error_iter)\n",
        "\n",
        "    _plot_losses(g_error_epoch, d_error_epoch)\n",
        "\n",
        "    # save the trained generator\n",
        "    torch.save(net_g.state_dict(),\n",
        "               \"models/generator.pth\")\n",
        "    # as well as the trained discriminator\n",
        "    torch.save(net_d.state_dict(),\n",
        "               \"models/discriminator.pth\")\n",
        "\n",
        "    return net_g, net_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qSd-gUGYZCWo"
      },
      "outputs": [],
      "source": [
        "def plot_fake_images(\n",
        "        generator, n_images: int = 9, _show: bool = False) -> None:\n",
        "    cols, rows = 3, 3\n",
        "    fig, axs = plt.subplots(rows, cols, sharex='all')\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    gen_z, label, _label_names = __generate_random_inputs(n_images)\n",
        "    gen_images = generator(gen_z, label)\n",
        "    images = gen_images.to(\"cpu\").clone().detach()\n",
        "    images = images.numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "    for i in range(9):\n",
        "        axs[i].set_title(_label_names[i])\n",
        "        axs[i].set_axis_off()\n",
        "        axs[i].imshow(images[i])\n",
        "    plt.tight_layout(pad=1.04)\n",
        "    plt.savefig(os.path.join('.img', 'fake_samples.png'), dpi=200)\n",
        "    if _show:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def __generate_random_inputs(n_images: int):\n",
        "    gen_z = torch.randn(n_images, nz, device=DEVICE)\n",
        "    label = torch.zeros(n_images, NUM_CLASSES, device=DEVICE)\n",
        "    _label_names = []\n",
        "    for i in range(n_images):\n",
        "        x = np.random.randint(0, NUM_CLASSES)\n",
        "        label[i][x] = 1\n",
        "        _label_names.append(LABEL_TO_TITLE[CLASS_TO_LABEL[x]])\n",
        "    return gen_z, label, _label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model():\n",
        "    print(\"Loading already trained and serialized generator model\")\n",
        "    generator = Generator().to(device(DEVICE))\n",
        "    generator.load_state_dict(torch.load(\n",
        "        os.path.join(\"models/generator.pth\")))\n",
        "    plot_fake_images(generator, _show=False)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ryKRfqa8a8Wf"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "#dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
        "dataset = CustomDataset(root_dir='../data/preprocessed_images', label_file='../data/odir_labels.csv', transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRiIj6xnZKBU",
        "outputId": "e6642fd2-bae6-47e8-f76a-82db83df6496"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de3f419c15bf4fb09898466cda026161",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHECKPOINT 4: saving some generated images at 'output/' directory\n",
            "CHECKPOINT4: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 9: saving some generated images at 'output/' directory\n",
            "CHECKPOINT9: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 14: saving some generated images at 'output/' directory\n",
            "CHECKPOINT14: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 19: saving some generated images at 'output/' directory\n",
            "CHECKPOINT19: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 24: saving some generated images at 'output/' directory\n",
            "CHECKPOINT24: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 29: saving some generated images at 'output/' directory\n",
            "CHECKPOINT29: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 34: saving some generated images at 'output/' directory\n",
            "CHECKPOINT34: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 39: saving some generated images at 'output/' directory\n",
            "CHECKPOINT39: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 44: saving some generated images at 'output/' directory\n",
            "CHECKPOINT44: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 49: saving some generated images at 'output/' directory\n",
            "CHECKPOINT49: saving the trained models at 'models/' directory\n",
            "CHECKPOINT 54: saving some generated images at 'output/' directory\n",
            "CHECKPOINT54: saving the trained models at 'models/' directory\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator, discriminator \u001b[38;5;241m=\u001b[39m train_gan(dataloader)\n\u001b[0;32m      2\u001b[0m plot_fake_images(generator, _show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[1;32mIn[33], line 30\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(data_loader, use_cpu, save_best_model, save_generated_images, verbose, _freq)\u001b[0m\n\u001b[0;32m     28\u001b[0m d_error_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     29\u001b[0m g_error_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# DISCRIMINATOR\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# train with real\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     net_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m     real_cpu \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
            "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     14\u001b[0m       img_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx])\n\u001b[1;32m---> 15\u001b[0m       image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_name)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m       image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     18\u001b[0m       \u001b[38;5;66;03m# Extract label from the label file\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "generator, discriminator = train_gan(dataloader)\n",
        "plot_fake_images(generator, _show=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kzWGcPP5dhDV"
      },
      "outputs": [],
      "source": [
        "# Pretrained ViT for Image Classification\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=weights)\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
