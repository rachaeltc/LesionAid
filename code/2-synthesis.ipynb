{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Image Synthesis\n",
    "OBJECTIVE: to create second training set with balanced classes through image synthesis techniques (diffusion model)\n",
    "- Load in training data\n",
    "- Use Diffusion Model to fill in gaps for each of 4 classes\n",
    "    - goal is 1600 samples for each class\n",
    "- Save new balanced dataset\n",
    "\n",
    "This notebook produces:\n",
    "* `train_synth_imgs.zip`: zip file with class-balanced dataset through image synthesis technique\n",
    "* `train_synth_labels.csv`: filename/labels for images in train_synth_imgs.zip\n",
    "* `diffusion_epochs.png`: image example of diffusion model outputs by epoch\n",
    "* `diffusion_class.png`: image example of synthetic images from each class, juxtaposed with original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\racha\\anaconda3\\envs\\cncomm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch import device\n",
    "from torch.cuda import is_available, device_count\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "DEVICE = device(\"cuda\" if is_available() else \"cpu\")  # (GPU if available)\n",
    "\n",
    "# Image and Label Constants\n",
    "IMAGE_SIZE: int = 64  # 128\n",
    "LABEL_TO_CLASS: dict = {\n",
    "    'D': 0,\n",
    "    'C': 1,\n",
    "    'M': 2,\n",
    "    'H': 3,\n",
    "}\n",
    "LABEL_TO_TITLE: dict = {\n",
    "    'D': \"Diabetes\",\n",
    "    'C': \"Cataract\",\n",
    "    'H': \"Hypertension\",\n",
    "    'M': \"Pathological Myopia\"\n",
    "}\n",
    "CLASS_TO_LABEL: dict = {_v: _k for _k, _v in LABEL_TO_CLASS.items()}\n",
    "NUM_CLASSES: int = len(LABEL_TO_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CustomDataset Class\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, root_dir, label_file, transform=None, label=\"ALL\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        if self.label == \"ALL\":\n",
    "          self.labels_df = pd.read_csv(label_file)\n",
    "        else: # only load images of the given label\n",
    "          label_df = pd.read_csv(label_file)\n",
    "          label_df = label_df[label_df['labels']==self.label].reset_index(drop=True)\n",
    "          self.labels_df = label_df\n",
    "        self.image_files = os.listdir(root_dir)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label from the label file\n",
    "        filename = self.image_files[idx]\n",
    "        if self.label=='ALL':\n",
    "          label = self.labels_df.loc[self.labels_df['filename'] == filename]['labels'].item()\n",
    "        else:\n",
    "          label = self.label\n",
    "        # Convert label to label class, to get index for encoding\n",
    "        label_ind = LABEL_TO_CLASS[label]\n",
    "        # one hot encoding for labels:\n",
    "        label_encoded = torch.zeros(NUM_CLASSES)\n",
    "        label_encoded[label_ind] = 1\n",
    "\n",
    "        return image, label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Model / Training Setup\n",
    "\n",
    "Code adapted from [Hugginface tutorial](https://huggingface.co/docs/diffusers/en/tutorials/basic_training):\n",
    "* training configuration\n",
    "* `UNet2DModel` architecture\n",
    "* Scheduler\n",
    "* Training\n",
    "* Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 64 # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"occularaid-M\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"cheungra/occularaid-M\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[0]\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "timesteps = torch.LongTensor([50])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "#dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
    "dataset = CustomDataset(root_dir='../data/preprocessed_images', label_file='../data/odir_labels.csv', transform=transform, label='M')\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LABELS_PATH = \"../data/train_labels.csv\"\n",
    "TRAIN_IMG_ZIP = \"../data/train_imgs.zip\"\n",
    "train_df = pd.read_csv(TRAIN_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_folder = \"train_synth_imgs\"\n",
    "os.makedirs(synth_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial test images, pre-repetition of samples\n",
    "for i, row in tqdm(train_df.iterrows()):\n",
    "  img = row['filename']\n",
    "  from_path = f'/content/preprocessed_images/{img}'\n",
    "  to_path = f'/content/{synth_folder}/{img}'\n",
    "  !cp $from_path $to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Images\n",
    "goal_samples = 1600\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "img_id = 0\n",
    "aug_filenames = []\n",
    "aug_labels = []\n",
    "\n",
    "for label in tqdm(labels):\n",
    "  # calculate number of images needed to be generated\n",
    "  cur_samples = list(train_df.labels).count(label)\n",
    "\n",
    "  to_generate = goal_samples - cur_samples\n",
    "  num_batches = to_generate // BATCH_SIZE\n",
    "\n",
    "  if to_generate < BATCH_SIZE: # if goal is met, no need to generate\n",
    "    continue\n",
    "\n",
    "  dataset = CustomDataset(root_dir='../data/preprocessed_images', label_file='../data/odir_labels.csv', transform=transform, label=label)\n",
    "  train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "  for _ in range(num_batches):\n",
    "    # load images of the current label\n",
    "    dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform, label=label)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    images, labels = next(iter(dataloader))\n",
    "    # save / track the images and labels\n",
    "    for img in images:\n",
    "      filename = f\"AUG_{img_id}.jpg\"\n",
    "\n",
    "      # save image to new directory\n",
    "      save_image(img, f'{aug_folder}/{filename}')\n",
    "      # keep track of new images\n",
    "      aug_filenames.append(filename)\n",
    "      aug_labels.append(label) # append letter label\n",
    "      img_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cncomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
