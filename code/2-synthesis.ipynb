{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Image Synthesis\n",
    "OBJECTIVE: to create second training set with balanced classes through image synthesis techniques (diffusion model)\n",
    "- Load in training data\n",
    "- Train 4 Diffusion Models to fill in gaps for each of 4 classes\n",
    "    - goal is 1600 samples for each class\n",
    "- Genearte synthetic data with diffusion models\n",
    "- Save new balanced dataset\n",
    "\n",
    "This notebook produces:\n",
    "* `train_synth_imgs.zip`: zip file with class-balanced dataset through image synthesis technique\n",
    "* `train_synth_labels.csv`: filename/labels for images in train_synth_imgs.zip\n",
    "* `diffusion_epochs.png`: image example of diffusion model outputs by epoch\n",
    "* `diffusion_class.png`: image example of synthetic images from each class, juxtaposed with original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch import device\n",
    "from torch.cuda import is_available, device_count\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "from dataclasses import dataclass\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from accelerate import notebook_launcher\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fabe5ca7614b62b29dd1c3e0be5a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "DEVICE = device(\"cuda\" if is_available() else \"cpu\")  # (GPU if available)\n",
    "\n",
    "# Image and Label Constants\n",
    "IMAGE_SIZE: int = 64  # 128\n",
    "LABEL_TO_CLASS: dict = {\n",
    "    'D': 0,\n",
    "    'C': 1,\n",
    "    'M': 2,\n",
    "    'H': 3,\n",
    "}\n",
    "LABEL_TO_TITLE: dict = {\n",
    "    'D': \"Diabetes\",\n",
    "    'C': \"Cataract\",\n",
    "    'H': \"Hypertension\",\n",
    "    'M': \"Pathological Myopia\"\n",
    "}\n",
    "CLASS_TO_LABEL: dict = {_v: _k for _k, _v in LABEL_TO_CLASS.items()}\n",
    "NUM_CLASSES: int = len(LABEL_TO_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CustomDataset Class\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, root_dir, label_file, transform=None, label=\"ALL\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        if self.label == \"ALL\":\n",
    "          self.labels_df = pd.read_csv(label_file)\n",
    "        else: # only load images of the given label\n",
    "          label_df = pd.read_csv(label_file)\n",
    "          label_df = label_df[label_df['labels']==self.label].reset_index(drop=True)\n",
    "          self.labels_df = label_df\n",
    "        self.image_files = os.listdir(root_dir)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label from the label file\n",
    "        filename = self.image_files[idx]\n",
    "        if self.label=='ALL':\n",
    "          label = self.labels_df.loc[self.labels_df['filename'] == filename]['labels'].item()\n",
    "        else:\n",
    "          label = self.label\n",
    "        # Convert label to label class, to get index for encoding\n",
    "        label_ind = LABEL_TO_CLASS[label]\n",
    "        # one hot encoding for labels:\n",
    "        label_encoded = torch.zeros(NUM_CLASSES)\n",
    "        label_encoded[label_ind] = 1\n",
    "\n",
    "        return image, label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Model / Training Setup\n",
    "\n",
    "Code adapted from [Hugginface tutorial](https://huggingface.co/docs/diffusers/en/tutorials/basic_training):\n",
    "* training configuration\n",
    "* `UNet2DModel` architecture\n",
    "* Scheduler\n",
    "* Training\n",
    "* Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 64 # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"occularaid-M\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"cheungra/occularaid-M\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[0]\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "#dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
    "dataset = CustomDataset(root_dir='../data/preprocessed_images', label_file='../data/odir_labels.csv', transform=transform, label='M')\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Training 4 UNet2DModels:\n",
    "* One model for each label\n",
    "* Model and Noise Scheduler get pushed to Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LABELS_PATH = \"../data/train_labels.csv\"\n",
    "TRAIN_IMG_FOLDER = \"../data/train_imgs\"\n",
    "train_df = pd.read_csv(TRAIN_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in tqdm(labels):\n",
    "    if label !='H':\n",
    "        continue\n",
    "    #dataset = CustomDataset(root_dir='/content/preprocessed_images', label_file='/content/drive/MyDrive/DS4440_Project/odir_labels.csv', transform=transform)\n",
    "    dataset = CustomDataset(root_dir=TRAIN_IMG_FOLDER, label_file=TRAIN_LABELS_PATH, transform=transform, label=label)\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "    #noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    timesteps = torch.LongTensor([50])\n",
    "    \"\"\"\n",
    "    model = UNet2DModel(\n",
    "        sample_size=config.image_size,  # the target image resolution\n",
    "        in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "        out_channels=3,  # the number of output channels\n",
    "        layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(64, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    )\"\"\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config.lr_warmup_steps,\n",
    "        num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    "    )\n",
    "\n",
    "    args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "    notebook_launcher(train_loop, args, num_processes=1)\n",
    "\n",
    "    #model.push_to_hub(f\"ocularaid-diffusion-{label}\")\n",
    "    #model.save_config(f'{label}_config.json')\n",
    "    n#oise_scheduler.push_to_hub(f\"ocularaid-scheduler-{label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "* Create new folder for synthetic images\n",
    "* Load trained models from Huggingface Hub\n",
    "* Generate appropraite number of images for each class and save\n",
    "* Update csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainA_df to store filename:label for synthetic dataset\n",
    "train_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "\n",
    "trainA_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a975c955a2c47428ee6be3e9c2364f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6266da9ce03846b19d45c02f05ca66ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/497 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kidje\\anaconda3\\envs\\rachaelenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kidje\\.cache\\huggingface\\hub\\models--cheungra--ocularaid-scheduler-D. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bac46b18e44a7da5049f80461a4c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate Images\n",
    "goal_samples = 1600\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "img_id = 0\n",
    "synth_filenames = []\n",
    "synth_labels = []\n",
    "\n",
    "SYNTH_IMG_FOLDER = \"../output/train_synth_imgs\"\n",
    "os.makedirs(SYNTH_IMG_FOLDER, exist_ok=True)\n",
    "\n",
    "for label in tqdm(labels):\n",
    "  # calculate number of images needed to be generated\n",
    "  cur_samples = trainA_df['labels'].value_counts()[label]\n",
    "\n",
    "  to_generate = goal_samples - cur_samples\n",
    "  num_batches = to_generate // BATCH_SIZE\n",
    "\n",
    "  if to_generate < BATCH_SIZE: # if goal is met, no need to generate\n",
    "    continue\n",
    "\n",
    "  repo_id = f\"cheungra/ocularaid-diffusion-{label}\"\n",
    "  my_model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n",
    "  noise_scheduler = DDPMScheduler.from_pretrained(f\"cheungra/ocularaid-scheduler-{label}\")\n",
    "\n",
    "  pipeline = DDPMPipeline(unet=Accelerator().unwrap_model(my_model), scheduler=noise_scheduler)\n",
    "\n",
    "  for _ in range(num_batches):\n",
    "    # load images of the current label\n",
    "    images = pipeline(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "    # save / track the images and labels\n",
    "    for img in images:\n",
    "      filename = f\"SYNTH_{img_id}.jpg\"\n",
    "\n",
    "      # save image to new directory\n",
    "      img.save(f'{SYNTH_IMG_FOLDER}/{filename}')\n",
    "      # keep track of new images\n",
    "      synth_filenames.append(filename)\n",
    "      synth_labels.append(label) # append letter label\n",
    "      img_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add synthetic labels to old df\n",
    "# create df for synthetic filename and labels\n",
    "synth_dict = {'filename': synth_filenames, 'labels': synth_labels}\n",
    "synth_df = pd.DataFrame(synth_dict)\n",
    "\n",
    "# merge the two dataframes containing aug and original data\n",
    "trainA_df = pd.concat([trainA_df, synth_df])\n",
    "\n",
    "# Check distribution:\n",
    "x = list(trainA_df.labels.unique())\n",
    "y_before = [trainA_df['labels'].value_counts()[label] for label in x]\n",
    "\n",
    "label_to_count = dict()\n",
    "for lab in zip(x, y_before):\n",
    "    label_to_count[lab[0]] = lab[1]\n",
    "\n",
    "print(label_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export new CSV\n",
    "trainA_df.to_csv('../data/train_synth_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cncomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
