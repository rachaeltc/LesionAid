<!doctype html>
<html lang="en">
<head>
<title>An Analysis of LesionAid</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">LesionAid: Vision Transformers-based Skin Lesion Generation and Classification</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="col">

<h2>Introduction</h2>
The main question we're tackling in our project is to examine how effectively we can apply an ocular lesion/ocular disease dataset to a solution proposed in the “LesionAid: Vision Transformers-based Skin Lesion Generation and Classification” by Krishna et al. Our data consists of images of ocular diseases labeled with one of eight classes, each representing a different ocular condition, and a ‘Normal’ class. Similar to the original paper, we aim to generate synthetic and augmented images, addressing class imbalance, and then employ a model for image classification on skin lesions. We follow a similar process to a paper but with different approaches at each step, with the goal to compare the effectiveness of the techniques. Specifically, we use a diffusion model for image generation instead of a ViTGAN, then performing similar image augmentation, and for image classification, we're using a pretrained ResNet on which we fine-tune our ocular lesion data.

<h2>Paper Review</h2>

This paper discusses the importance of early detection of skin lesions in the dermatology field and gives a review and analysis of the use of advanced deep learning techniques to help aid in the early detection as well as the classification of certain skin lesions. It starts off discussing the challenges behind utilizing these techniques, more specifically the challenges associated with Generative Adversarial Networks (GANs) for data augmentation as well as Visual Transformers (ViT) for image classification. The experiments outlined in this article utilize the HAMS10000 dataset to explore different augmentation techniques as well as different performance evaluation metrics such as the Frechet Inception Distance (FID). It aims to find how effective a ViT model can be for the classification of different skin lesions and diseases for multiple different classes at once as well as using explainable AI methods like GradCAM to interpret different models. 
<br /><br />
For the sake of our project we focus mainly on the use of GANs for data augmentation and the ViT for image classification as the paper helped us come to the conclusion that these techniques were very helpful in achieving high accuracy rates and different challenges that individuals face in using deep learning techniques such as class imbalance. All in all, the paper discusses the potential importance of deep learning techniques in the medical field, specifically in dermatology diagnostics, because with the use of a ViTGAN model it becomes increasingly accurate in the recognition and classification of different skin diseases.

<h2>Method</h2>
Our implementation aims to mimic the process of that used in the original paper, which consists of three phases: image synthesis, image augmentation, and image classification. 
<br /><br /><b>Image Synthesizing: Diffusion Model and Image Augmentation </b><br />
	In the first component, image synthesis, we aim to generate synthetic images for different labels of ocular lesions, with the intent of mitigating class imbalance. For this purpose, we train a UNet2DModel from scratch, through Huggingface. THe Unet2DModel is based on the UNet Model, which was developed with the motivation of being able to successfully train deep networks with fewer training samples. They achieve this through a U-shaped architecture, consisting of an encoder, the contracting path, and the decoder, the expanding path. One of the main benefits of this model is that it can output images that are the same size as the input, and was successfully tested on biomedical image segmentation. 
  <img src="viz/unet.png" alt="UNet" class="center", width="45%"/>
  The image data is first loaded into a Custom Dataset class, which resizes (to 64x64) and normalizes the images to become the training data for this model. The purpose of the UNet2DModel is to denoise the input samples– it takes in a noisy sample and a timestep, and returns a sample-shaped output. We trained a separate diffusion model for each of the eight classes of our data, and used these models to generate synthetic ocular images to be prepared as training data for image classification. 
	<br /> 
  We also employed image augmentation techniques, from the torchvision transforms library, to augment original images to create more samples within classes that were lacking samples. This included random rotation, random cropping, horizontal and vertical flip, as well as  variations in brightness, saturation, and hue. 
<br /><br /><b>Vision Transformer for Image Classification</b> <br />
	For the Vision Transformer for Image Classification we created a SyntheticDataset class which was meant to be used to generate the synthetic images for training. This class helped initialize the dataset, providing the total number of samples in the dataset, as well as the actual generation of synthetic images as well as their corresponding labels by the generator function. From there, an instance of this class is create with the previously trained generator and wrapped in a data loader so that it can iterate over all of the different shuffled batches of data throughout the training process. In terms of the actual model, we used a pre-trained ResNet50, a convolutional neural network architecture, model for the image classification. From there cross-entropy loss is used as the loss function to optimize the model followed by the actual training loop for the image classification. That being said, because we struggled with incredibly long run times with run times for the training loop.
<br/><br/>
Code for the implementation can be found <a href='https://github.com/rachaeltc/LesionAid/'>here</a> 


<h2>Findings</h2>
<img src="viz/class_imbalance.png" alt="ODIR-5k Class Imbalance" class="center", width="50%"/>

We can see that the original dataset contained a disproportionate amount of the ‘N’ class, with over 2500 samples, compared to the ‘M’, ‘H’, ‘A’, ‘G’ classes, which have less than 300 each. After performing image synthesis and augmentation, we can produce more balanced classes with the artificial data. 
<br /> <br />
Here are some examples of images generated through the diffusion model, through the epochs:
<img src="viz/diffusion_epochs.png" alt="ODIR-5k Class Imbalance" class="center", width="75%"/>

<br/>[FID Score Comparison by Label]

<br/><br/>
[Image Classification Results in Progress]
<br/> to add Accuracy vs Epoch graph, Epoch vs Loss trend graph

<h2>Conclusion</h2>
[conclusion]

<h2>References</h2>

<p><a name="krishna-2023">[1]</a>
    Ghanta Sai Krishna and Kundrapu Supriya and Mallikharjuna Rao K and Meetiksha Sorgile. 
  <em>LesionAid: Vision Transformers-based Skin Lesion Generation and Classification.</em> 
  <a href="https://arxiv.org/ftp/arxiv/papers/2302/2302.01104.pdf">arXiv:2302.01104</a>, February 2023.
</p>
<p><a name="krishna-2023">[2]</a>
  Olaf Ronneberger and
  Philipp Fischer and
  Thomas Brox. 
<em>U-Net: Convolutional Networks for Biomedical Image Segmentation.</em> 
<a href="https://arxiv.org/abs/1505.04597">arXiv:1505.04597</a>, May 2015.
</p>



<h2>Team Members</h2>
                                                   
<p>Sarah Casale, Rachael Cheung</p>

  
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/">About DS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
